{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:200%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#fc6603'>AUTHOR: SOBIA ALAMGIR</span></b></p></div>\n",
    "\n",
    "<a id=\"13\"></a>\n",
    "<h1 style=\"background-color:#435420;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;color:#FF9900;\">Hyperparameter Tuning</h1>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Hyperparameter tuning** is choosing the best settings for a model (like the depth of a tree or the learning rate) to make sure it works as well as possible on new data.\n",
    "\n",
    "## **Types**:\n",
    "\n",
    "- `Grid Search:` Tests all possible combinations of specified hyperparameter values to find the best one, like an exhaustive search.\n",
    "  \n",
    "- `Random Search:` Randomly samples hyperparameter combinations from a specified range, which is faster for large search spaces and can still find optimal settings effectively.\n",
    "\n",
    "- `Bayesian Optimization:` Uses probabilistic models (like Gaussian processes) to predict the best hyperparameters by learning from previous trials, aiming to efficiently find the optimal combination.\n",
    "\n",
    "- `Gradient-Based Optimization:` Uses gradients to adjust hyperparameters (similar to gradient descent for weights) to converge on the best values, typically used in neural networks (e.g., learning rate scheduling).\n",
    "\n",
    "## **Key Concepts:**\n",
    "\n",
    "- **Cross-validation** is a method to assess model performance by dividing the dataset into multiple parts, training on some parts, and validating on others. The most common type, **k-fold cross-validation**, splits the data into **k** folds, ensuring each fold is used as validation once to reduce overfitting and provide a reliable evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-01 Load Libraries using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score, classification_report\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-02 Load Data and Target using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-03 Hyperparameter tuning using `Grid Search CV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "Best Parameters:{'bootstrap': True, 'criterion': 'gini', 'max_depth': 4, 'n_estimators': 50}\n",
      "CPU times: total: 4.8 s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# create the parameter Grid\n",
    "params = {'n_estimators': [50,100,200,300,400,500],\n",
    "          'max_depth': [4,5,6,7,8,9,10],\n",
    "          'criterion':['gini','entropy'],\n",
    "          'bootstrap':[True,False]\n",
    "          }\n",
    "\n",
    "# setup the grid with `Grid Search CV`\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=params,\n",
    "    cv = 5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid.fit(X,y)\n",
    "\n",
    "# Print the best Parameters\n",
    "print(f'Best Parameters:{grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Let's save the model `Grid Search CV`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump('grid','Decision_tree_classifier_with_GridSearchCV.pkl')\n",
    "load_model_with_GridSearchCV = joblib.load('Decision_tree_classifier_with_GridSearchCV.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-04 Hyperparameter Tuning using `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters:{'n_estimators': 100, 'max_depth': 8, 'criterion': 'entropy', 'bootstrap': True}\n",
      "CPU times: total: 797 ms\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# create the parameter Grid\n",
    "params = {'n_estimators': [50,100,200,300,400,500],\n",
    "          'max_depth': [4,5,6,7,8,9,10],\n",
    "          'criterion':['gini','entropy'],\n",
    "          'bootstrap':[True,False]\n",
    "          }\n",
    "\n",
    "# setup the grid with `Random Search CV`\n",
    "grid_random = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=params,\n",
    "    cv = 5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_iter = 20\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_random.fit(X,y)\n",
    "\n",
    "# Print the best Parameters\n",
    "print(f'Best Parameters:{grid_random.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "\n",
    "- `Randomized Search CV` is less time taken as compare to `Grid Search CV`\n",
    "- Randomized Search CV choosed `random` 100 fits, `execute 100 times/combination`.\n",
    "- Grid Search CV executes 840 times to fit the model, `executes All possible combinations.`\n",
    "  \n",
    "  - ***Grid Search CV is more `appropriate` than any other kind of search objectors***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Let's save the model `Randomized Search CV`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump('grid_random', 'Decision_tree_classier_with randomSearchCV.pkl')\n",
    "load_model_with_random_search = joblib.load('Decision_tree_classier_with randomSearchCV.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"13\"></a>\n",
    "<h1 style=\"background-color:#435420;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;color:#FF9900;\">Thanks For Reading My Notebook!â€‹</h1>\n",
    "<figcaption style=\"text-align: center;\">\n",
    "    <strong>\n",
    "    </strong>\n",
    "</figcaption>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
